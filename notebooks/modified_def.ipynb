{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b55e67",
   "metadata": {},
   "source": [
    "# Modified Definition Test\n",
    "This notebook seeks to test the modified definition to the disruptivity that I defined using chains of events. I am worried this definition will suck due to the variance of the Poisson distribution but who knows! The new method is outlined in this blog post:\n",
    "\n",
    "https://cfsenergy.atlassian.net/wiki/spaces/~6318cff19794410874c7744f/blog/2023/05/05/2788819001/Lecture+2+fr+Froude+3+Probabilities+3+7+05+2023\n",
    "\n",
    "We start with imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d30f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "from scipy.interpolate import interp1d, LinearNDInterpolator\n",
    "\n",
    "# Move into the source directory for this notebook to work properly\n",
    "# Probably want a better way of doing this.\n",
    "import os\n",
    "import importlib\n",
    "os.chdir('../src/')\n",
    "\n",
    "# Import whatever we need\n",
    "import disruptivity as dis\n",
    "import vis.disruptivity_vis as dis_vis\n",
    "import vis.probability_vis as prob_vis\n",
    "from vis.plot_helpers import plot_subplot as plot\n",
    "import data_loader\n",
    "\n",
    "# Import tokamak Configuartions\n",
    "from tokamaks.cmod import CONFIG as CMOD\n",
    "from tokamaks.d3d import CONFIG as D3D\n",
    "\n",
    "importlib.reload(dis)\n",
    "importlib.reload(dis_vis)\n",
    "load_disruptions_mat = data_loader.load_disruptions_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ee21e",
   "metadata": {},
   "source": [
    "Loading is the same as before, we use the premade functions for disruptivity computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmod_df, cmod_indices = load_disruptions_mat('../data/CMod_disruption_warning_db.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2345168",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "So my goal with this block of code is to find all the portions of flat top disrupted shots \n",
    "that are in flat tops. Should be simple enough.\n",
    "'''\n",
    "\n",
    "# Entry dictionary\n",
    "entry_dict_1D = {\n",
    "    'z_error': CMOD[\"entry_dict\"][\"z_error\"],\n",
    "    'kappa': CMOD[\"entry_dict\"][\"kappa\"],\n",
    "#     'z_error': CMOD[\"entry_dict\"][\"z_error\"],\n",
    "}\n",
    "\n",
    "entry_dict_2D = {\n",
    "    'q95':CMOD['entry_dict']['q95'],\n",
    "    'n_e':CMOD['entry_dict']['n_e'],\n",
    "}\n",
    "\n",
    "# Hugill\n",
    "# Compute the murakami parameter\n",
    "cmod_df['inv_q95'] = 1/cmod_df['q95']\n",
    "cmod_df['murakami'] = cmod_df['n_e']*0.68/(cmod_df['n_equal_1_mode']/cmod_df['n_equal_1_normalized'])/1e19\n",
    "\n",
    "entry_dict_H = {\n",
    "    'murakami':{\n",
    "        'range':[0,20],\n",
    "        'axis_name': \"$n_e R/B_T \\ (10^{19}$m$^{-2}$/T)\",\n",
    "    },\n",
    "    'inv_q95':{\n",
    "        'range':[0, 1],\n",
    "        'axis_name': \"$1/q_{95}$\",\n",
    "    },\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10ead6",
   "metadata": {},
   "source": [
    "Now, we can reuse the histogram binning code for variable timesteps that returns the data indices of data points for each bin. Since this new method essentially tries to compute the dt of subsequent data points, it is mechanically the same as the dt calculation for variable timestep as well! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd9b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# Computing the disruptivity and plotting the regular figure\n",
    "# shotlist=None\n",
    "# entry_dict = entry_dict_2D\n",
    "# indices_n_disrupt, indices_n_total = dis.get_indices_disruptivity(CMOD, cmod_df, cmod_indices, shotlist=shotlist)\n",
    "# args = dis.compute_disruptivity_likelihood(cmod_df, entry_dict, indices_n_total, nbins=35, tau=50, window=25)\n",
    "fig,ax = plot('cmod_q95_ne_disruptivity_kaloyannis.png', dis_vis.subplot_disruptivity2d, args)\n",
    "\n",
    "# Get a pulse's flat top data\n",
    "shot = 1140226013 #AT PULSE\n",
    "# shot = 1120105021 #VDE\n",
    "draw_trajectory(ax,cmod_df,entry_dict,cmod_indices,shot)\n",
    "# Plotting Constraints\n",
    "# ax.plot([0,20], [0.5, 0.5], '--', c='orange')\n",
    "# ax.plot([0,20], [0.0, 0.5], '--', c='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336081fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_vis.plot_data_selection(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac41bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_crunch(dataframe, indices, shotlist, tokamak, figtype, nbins=25, tau=0, window=2, dis_type='sampling'):\n",
    "    \n",
    "    # Figure settings\n",
    "    figwidth = 6\n",
    "    figheight = 4\n",
    "    \n",
    "    # Create the big plot\n",
    "    n_plots = len(tokamak['entry_dict'])\n",
    "    n_cols = 4\n",
    "    # Make sure we don't make stupid plots that are all whitespace\n",
    "    if n_plots < n_cols:\n",
    "        n_cols = n_plots\n",
    "    n_rows = np.ceil(n_plots/n_cols).astype(int)\n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(\n",
    "        figwidth*n_cols, figheight*n_rows), constrained_layout=True)\n",
    "    fig2, ax2 = plt.subplots(n_rows, n_cols, figsize=(\n",
    "        figwidth*n_cols, figheight*n_rows), constrained_layout=True)\n",
    "    \n",
    "    # Compute the index histograms\n",
    "    indices_n_disrupt, indices_n_total = dis.get_indices_disruptivity(tokamak, dataframe, indices, shotlist=shotlist, tau=tau, window=window)\n",
    "    \n",
    "    # Loop through the data fields\n",
    "    # If we only have a single row\n",
    "    for (i, entry) in enumerate(tokamak['entry_dict']):\n",
    "        # Information Lines\n",
    "        print(\"Working on \"+entry)\n",
    "\n",
    "        # Create the entry dict\n",
    "        entry_dict = {entry:tokamak['entry_dict'][entry]}\n",
    "\n",
    "        # Get the tokamak name\n",
    "        name = tokamak['name']\n",
    "\n",
    "        # Compute Disruptivity and save the plot\n",
    "        if dis_type=='sampling':\n",
    "            args = dis.compute_disruptivity_sampling(dataframe,\n",
    "                                            entry_dict,\n",
    "                                            indices_n_disrupt,\n",
    "                                            indices_n_total,\n",
    "                                            nbins=nbins)\n",
    "        \n",
    "        elif dis_type=='likelihood':\n",
    "            args = dis.compute_disruptivity_likelihood(dataframe,\n",
    "                                                       entry_dict,\n",
    "                                                       indices_n_total,\n",
    "                                                       nbins=nbins,\n",
    "                                                       tau = tau,\n",
    "                                                       window = window,\n",
    "                                                      )\n",
    "        else:\n",
    "            assert False, f\"Invalid calculation type {dis_type}\"\n",
    "            \n",
    "        dis_vis.subplot_disruptivity1d(ax.flat[i], *args)\n",
    "\n",
    "    # Remove axes of unrendered plots\n",
    "    for i in range(n_plots, n_cols*n_rows):\n",
    "        ax.flat[i].axis('off')\n",
    "                \n",
    "    return fig,ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the big crunch\n",
    "cmod_vde_shotlist = np.loadtxt(\"../data/cmod_vde_shotlist.txt\", dtype=int)\n",
    "\n",
    "# Parameter setup\n",
    "# figtype = 'disruptivity_vde_kaloyannis'\n",
    "figtype = 'disruptivity_likelihood'\n",
    "shotlist = None # set to None for no shotlist\n",
    "\n",
    "# Compute indices of interest\n",
    "indices_n_disrupt, indices_n_total = dis.get_indices_disruptivity(CMOD, cmod_df, cmod_indices, shotlist=shotlist)\n",
    "\n",
    "fig,ax = big_crunch(cmod_df, cmod_indices, shotlist, CMOD, figtype, nbins=25, tau=50, window=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"big_crunch.png\", dpi=400, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cae52d",
   "metadata": {},
   "source": [
    "### Boundary avoidance time\n",
    "\n",
    "Soooo last time we did boundary avoidance we were using scipy stuff for 1d interpolator. I want to move away from that since we have the ability to compute these maps in N dimensions. Now the question is how do we do this. I think that the way to do it is to create a mask value that ummm fills the gaps in the data with a high disruptivity value. This of course implies that the disruptivity maps will push the control system away form unexplored regions. I don't know if we should do that in practice but it is the first thing i thought of and it is what I want to do ok so don't @ me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a disruptivity map of N dimensions and fill unvisited regions with a high disruptivity.\n",
    "# This means we need a sepearate mask for regions with no data and regions with effectively 0 disruptivity. \n",
    "# We want to avoid the situation where the disruptivity is so low that there are functionally no disruptions\n",
    "# and having the control system avoid these hyper stable scenarios.\n",
    "d_array = args[0]\n",
    "bin_centers = (np.array(args[2])[:,1:]+np.array(args[2])[:,:-1])/2\n",
    "\n",
    "# N dimensional meshgrid creation from the bin centers\n",
    "xx = np.meshgrid(*bin_centers)\n",
    "\n",
    "# NDINTERPOLATOR\n",
    "# First, treat the masked values\n",
    "# No data is the max, no disruptions is min\n",
    "# All disruptions is max.\n",
    "interpolator_array = np.copy(d_array)\n",
    "d_min = d_array[d_array>0].min()\n",
    "d_max = d_array[d_array>0].max()\n",
    "interpolator_array[d_array==-1]= d_max\n",
    "interpolator_array[d_array==-2]= d_min\n",
    "interpolator_array[d_array==-3]= d_max\n",
    "print(\"Max Fill: \",d_max, \"Min Fill:\", d_min)\n",
    "\n",
    "# Now do the fill value being the maximum\n",
    "interper = scipy.interpolate.RegularGridInterpolator(bin_centers, interpolator_array,\n",
    "                                                     method='cubic',bounds_error=False,\n",
    "                                                     fill_value=d_array.max())\n",
    "\n",
    "# Visualize the filling\n",
    "args2 = list(args)\n",
    "args2[0]=interpolator_array\n",
    "fig,ax = plot('vis_fill.png', dis_vis.subplot_disruptivity2d, args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Iterative averaging procedure\n",
    "Hold real data, no data, all disrupted as fixed values\n",
    "Iteratively loop:\n",
    "    Conv2D with averaging kernel of size n.\n",
    "    Replace the fixed values, \n",
    "    Check for convergence with last iteration via relative_tol (bin wise).\n",
    "    Check for max iterations\n",
    "'''\n",
    "\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Setup\n",
    "# d_max_mult is the multiplier assigned to\n",
    "# no_data, disrupted data and out of bounds data\n",
    "n_dims = len(args[2])\n",
    "d_array = args[0]\n",
    "d_max_mult = 1\n",
    "max_iter=100\n",
    "rel_tol = 1e-6\n",
    "n = 11\n",
    "\n",
    "# Create the fixed points array\n",
    "fixed_points = np.copy(d_array)\n",
    "max_fill = d_array[d_array>0].max()*d_max_mult\n",
    "fixed_points[d_array==-1]= max_fill\n",
    "fixed_points[d_array==-3]= max_fill\n",
    "\n",
    "# Create the iter_array\n",
    "iter_array = np.zeros(fixed_points.shape)\n",
    "iter_array[d_array!=-2] = fixed_points[d_array!=-2]\n",
    "\n",
    "# Create the convolutional filter\n",
    "ave_filter = np.ones([n]*n_dims)/(n**n_dims)\n",
    "\n",
    "# Iterate\n",
    "for i in range(max_iter):\n",
    "    # Step 1: Convolve\n",
    "    new_iter_array = convolve2d(iter_array,ave_filter,\n",
    "                                mode='same', \n",
    "                                fillvalue=max_fill)\n",
    "    \n",
    "    # Step 2: Replace the fixed points\n",
    "    new_iter_array[d_array!=-2] = fixed_points[d_array!=-2]\n",
    "    \n",
    "    # Step 3: Relative Tolerance Check\n",
    "    # Check for division by 0s in early cycles\n",
    "    if (new_iter_array!=0).all():\n",
    "        residuals = abs(new_iter_array-iter_array)/new_iter_array\n",
    "        if (residuals<=rel_tol).all():\n",
    "            print(f'Data filling converged after {i} iterations.')\n",
    "            break\n",
    "    \n",
    "    # Save the new iteration\n",
    "    iter_array = new_iter_array\n",
    "    \n",
    "# Run one last smoothing operation on the data with a small Gaussian Filter\n",
    "iter_array = gaussian_filter(iter_array, sigma=0.5, truncate=3)\n",
    "\n",
    "# Prep the interpolator\n",
    "bin_centers = (np.array(args[2])[:,1:]+np.array(args[2])[:,:-1])/2\n",
    "xx = np.meshgrid(*bin_centers)\n",
    "interper = scipy.interpolate.RegularGridInterpolator(bin_centers, iter_array,\n",
    "                                                     method='linear',\n",
    "                                                     bounds_error=False,\n",
    "                                                     fill_value=max_fill)\n",
    "    \n",
    "# Visualize the filling\n",
    "args2 = list(args)\n",
    "args2[0]=iter_array\n",
    "fig,ax = plot('iter_fill.png', dis_vis.subplot_disruptivity2d, args2)\n",
    "\n",
    "# Get a pulse's flat top data\n",
    "# shot = 1140226013 # AT Pulse\n",
    "shot = 1160930030 \n",
    "# shot = 1120105021 #VDE\n",
    "draw_trajectory(ax,cmod_df,entry_dict,cmod_indices,shot)\n",
    "# ax.plot([0,20], [0.5, 0.5], '--', c='red')\n",
    "# ax.plot([0,20], [0.0, 0.5], '--', c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e307f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's bring over some bicubic interpolation stuff.\n",
    "def cubic(dx,f):\n",
    "    \"\"\"Computes the cubic interpolation given the lookup vector f.\n",
    "    \n",
    "    Inputs: f (np.ndarray (1x4)) the lookup vector.\n",
    "            dx (double) the dx value in range of [0,1].\n",
    "            \n",
    "    Returns: ans (double), the interpolation value.\n",
    "    \"\"\"\n",
    "    mat = [[0, 2, 0, 0],\n",
    "       [-1, 0, 1, 0],\n",
    "       [2, -5, 4, -1],\n",
    "       [-1, 3, -3, 1]]\n",
    "    mat = np.array(mat)/2.0\n",
    "    dx_vec = np.array([[1, dx, dx**2, dx**3]])\n",
    "    \n",
    "    return (dx_vec@mat@f)[0]\n",
    "\n",
    "def cubic_deriv(dx, f):\n",
    "    \"\"\"Computes the cubic derivative given the lookup vector f.\n",
    "\n",
    "    Inputs: f (np.ndarray (1x4)) the lookup vector.\n",
    "            dx (double) the dx value in range of [0,1].\n",
    "\n",
    "    Returns: ans (double), the interpolation value.\n",
    "    \"\"\"\n",
    "    mat = [[0, 2, 0, 0],\n",
    "           [-1, 0, 1, 0],\n",
    "           [2, -5, 4, -1],\n",
    "           [-1, 3, -3, 1]]\n",
    "    mat = np.array(mat)/2.0\n",
    "    dx_vec = np.array([[0, 1, 2*dx, 3*dx**2]])\n",
    "    \n",
    "    return (dx_vec@mat@f)[0]\n",
    "\n",
    "def bicubic(dx, dy, f):\n",
    "    \"\"\"Computes the bicubic interpolation given the lookup matrix f.\n",
    "    \n",
    "    Inputs: f (np.ndarray (4x4)) the lookup matrix.\n",
    "            dx (double) the dx value in range of [0,1].\n",
    "            dy (double) the dy value in range of [0,1].\n",
    "            \n",
    "    Returns: ans (double), the interpolation value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Complete 4 1D cubics over dx.\n",
    "    f_y = np.array([cubic(dx, f[i]) for i in range(4)])\n",
    "    \n",
    "    # Compute the 1D cubic over dy.\n",
    "    return cubic(dy,f_y)\n",
    "\n",
    "def bicubic_grad(dx, dy, f):\n",
    "    \"\"\"Computes the bicubic interpolation gradient given the lookup matrix f.\n",
    "    \n",
    "    Inputs: f (np.ndarray (4x4)) the lookup matrix.\n",
    "            dx (double) the dx value in range of [0,1].\n",
    "            dy (double) the dy value in range of [0,1].\n",
    "            \n",
    "    Returns: ans (double), the gradient value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Complete 4 1D cubics over dx.\n",
    "    f_y = np.array([cubic(dx, f[i]) for i in range(4)])\n",
    "\n",
    "    # Complete 4 1D cubics over dx derivative.\n",
    "    f_y_deriv = np.array([cubic_deriv(dx, f[i]) for i in range(4)])\n",
    "\n",
    "    # Compute the 1D cubics over dy and dy derivative.\n",
    "    # Returns [d/dx, d/dy]\n",
    "    return [cubic(dy, f_y_deriv), cubic_deriv(dy, f_y)]\n",
    "    \n",
    "def get_dx_dy_f(x_grid, y_grid, f_padded, x, y):\n",
    "    \"\"\"Get the dx, dy, and f for an interpolation matrix f.\n",
    "    NEEDS TESTING BUT SHOULD BE OK.\n",
    "    \n",
    "    Inputs: stuff\n",
    "    \n",
    "    Returns: stuff\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the index to the right of the pt\n",
    "    # Notice that f_padded is padded by 0s\n",
    "    # on either side, meaning that what would\n",
    "    # usually be idx_2 is instead idx_0 or the\n",
    "    # start index of the search\n",
    "    idx_0 = np.searchsorted(x_grid, x)\n",
    "    idy_0 = np.searchsorted(y_grid, y)\n",
    "\n",
    "    # dx, dy\n",
    "    x_res = x_grid[1]-x_grid[0]\n",
    "    dx = (x - (x_grid[idx_0]-x_res))/x_res\n",
    "    y_res = y_grid[1]-y_grid[0]\n",
    "    dy = (y - (y_grid[idy_0]-y_res))/y_res\n",
    "    \n",
    "    # Array\n",
    "    f = f_padded[idy_0:idy_0+4,idx_0:idx_0+4]\n",
    "    \n",
    "    return dx, dy, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa65b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_padded = np.pad(iter_array, pad_width=2, mode='edge')\n",
    "get_dx_dy_f(x_grid, y_grid, f_padded, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5dfabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok so now lets look at probability of disruption for a pulse\n",
    "# shot = 1140226013 # AT Pulse\n",
    "# shot = 1120105021 #VDE\n",
    "data = np.array(cmod_df[entry_dict.keys()][cmod_df.shot == shot])\n",
    "y = dis.p_data(interper(data),1,True)\n",
    "\n",
    "# Assume the last data point is the disruption time\n",
    "disr_index = cmod_df.time_until_disrupt.index[cmod_df.shot == shot][-1]\n",
    "\n",
    "plt.plot(cmod_df.time[cmod_df.shot == shot],y, label=str(shot))\n",
    "plt.plot([cmod_df.time[disr_index],cmod_df.time[disr_index]],[0,1], '--', color='orange', label=\"Disruption Time\")\n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Disruption Probability\")\n",
    "plt.ylim(-0,1.)\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390953be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean time to disruption as a function of time?\n",
    "plt.plot(cmod_df.time[cmod_df.shot == shot],cmod_df.time[cmod_df.shot == shot]+1/interper(data), label=str(shot)+\" $1/d+t_{pulse}$\")\n",
    "plt.plot(cmod_df.time[cmod_df.shot == shot], 1/interper(data),linestyle='dashdot', label=str(shot)+\" $1/d$\" , color='red')\n",
    "plt.plot([cmod_df.time[disr_index],cmod_df.time[disr_index]],[0,100], '--', color='orange', label=\"Disruption Time\")\n",
    "plt.plot([0,1.5],[cmod_df.time[disr_index],cmod_df.time[disr_index]], '--', color='orange')\n",
    "\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Predicted Disruption Time (s)\")\n",
    "plt.ylim(0,15)\n",
    "plt.xlim(0,1.5)\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5cf431",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/interper(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9452a",
   "metadata": {},
   "source": [
    "### DIII-D Crunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb60b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokamaks.d3d import CONFIG as D3D\n",
    "d3d_df, d3d_indices = load_disruptions_mat('../data/d3d-db-220420.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter setup\n",
    "figtype = 'disruptivity_likelihood'\n",
    "shotlist = None # set to None for no shotlist\n",
    "     \n",
    "# Compute indices of interest\n",
    "indices_n_disrupt, indices_n_total = dis.get_indices_disruptivity(D3D, d3d_df, d3d_indices, shotlist=shotlist)\n",
    "\n",
    "big_crunch(d3d_df, indices_n_total, shotlist, D3D, figtype, nbins=35, tau=350, window=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d3d_df['z_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df07615",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cmod_df['shot'][446000:475000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmod_df['shot'][465000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def subplot_trajectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c9896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmod_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(cmod_df['n_e']/1e19)\n",
    "test[test>70] = 0\n",
    "test[test<0] = 0\n",
    "plt.plot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f427ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cmod_df['n_e']/1e19, range=[0,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95722bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmod_vde_shotlist = np.loadtxt(\"../data/cmod_vde_shotlist.txt\", dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmod_vde_shotlist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831531c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68cc76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
