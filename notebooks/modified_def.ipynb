{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b55e67",
   "metadata": {},
   "source": [
    "# Modified Definition Test\n",
    "This notebook seeks to test the modified definition to the disruptivity that I defined using chains of events. I am worried this definition will suck due to the variance of the Poisson distribution but who knows! The new method is outlined in this blog post:\n",
    "\n",
    "https://cfsenergy.atlassian.net/wiki/spaces/~6318cff19794410874c7744f/blog/2023/05/05/2788819001/Lecture+2+fr+Froude+3+Probabilities+3+7+05+2023\n",
    "\n",
    "We start with imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d30f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "\n",
    "# Move into the source directory for this notebook to work properly\n",
    "# Probably want a better way of doing this.\n",
    "import os\n",
    "import importlib\n",
    "os.chdir('../src/')\n",
    "\n",
    "# Import whatever we need\n",
    "import disruptivity as dis\n",
    "import indexing as ind\n",
    "import vis.disruptivity_vis as dis_vis\n",
    "import vis.probability_vis as prob_vis\n",
    "from vis.plot_helpers import plot_subplot as plot\n",
    "import data_loader\n",
    "\n",
    "# Import tokamak Configuartions\n",
    "from tokamaks.cmod import CONFIG as CMOD\n",
    "from tokamaks.d3d import CONFIG as D3D\n",
    "\n",
    "importlib.reload(ind)\n",
    "importlib.reload(dis)\n",
    "importlib.reload(dis_vis)\n",
    "load_disruptions_mat = data_loader.load_disruptions_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ee21e",
   "metadata": {},
   "source": [
    "Loading is the same as before, we use the premade functions for disruptivity computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f9ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmod_df, cmod_indices = load_disruptions_mat('../data/CMod_disruption_warning_db.mat')\n",
    "n_shots = np.unique(cmod_df.shot).shape[0]\n",
    "n_shots_no_disrupt = np.unique(cmod_df.shot[cmod_indices['indices_no_disrupt']]).shape[0]\n",
    "n_shots_disrupt = np.unique(cmod_df.shot[cmod_indices['indices_disrupt']]).shape[0]\n",
    "assert n_shots_disrupt+n_shots_no_disrupt == n_shots, \\\n",
    "    'Number of disrupts plus number of non disruptions does not equal the total shot number'\n",
    "print(f'Total Shot Number: {n_shots}, Non-Disrupted Shots: {n_shots_no_disrupt}, Disrupted Shots: {n_shots_disrupt}')\n",
    "\n",
    "'''\n",
    "So my goal with this block of code is to find all the portions of flat top disrupted shots \n",
    "that are in flat tops. Should be simple enough.\n",
    "'''\n",
    "\n",
    "# Compute the indice\n",
    "ind.get_indices_disruptivity(CMOD, cmod_df, cmod_indices)\n",
    "ind.get_indices_detectable_disruptivity(CMOD, cmod_df, cmod_indices)\n",
    "\n",
    "\n",
    "# All other flattop data points\n",
    "indices_n_detectable_disrupt = cmod_indices['indices_n_detectable_disrupt']\n",
    "indices_n_detectable_total = cmod_indices['indices_n_detectable_total']\n",
    "indices_n_disrupt = cmod_indices['indices_n_disrupt']\n",
    "indices_n_total = cmod_indices['indices_n_total']\n",
    "\n",
    "# Entry dictionary\n",
    "entry_dict_1D = {\n",
    "    'kappa':{\n",
    "        'range':[0.8, 2.0],\n",
    "        'axis_name': \"$\\kappa$\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10ead6",
   "metadata": {},
   "source": [
    "Now, we can reuse the histogram binning code for variable timesteps that returns the data indices of data points for each bin. Since this new method essentially tries to compute the dt of subsequent data points, it is mechanically the same as the dt calculation for variable timestep as well! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc59dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the histogram with the list of data entries.\n",
    "# There is no need for numerator and denominators, only histograms of all the data.\n",
    "hist = dis.indices_to_histogram(cmod_df, entry_dict_1D, indices_n_total, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a modified variable dt function that stores the sequence of dts and marks them as disrupted or not.\n",
    "def compute_dt_bin(\n",
    "    dataframe: pd.core.frame.DataFrame,\n",
    "    denom_dd: scipy.stats._binned_statistic.BinnedStatisticddResult,\n",
    "    denom_indices: np.ndarray,\n",
    "    tau = 0,\n",
    "    window = 1,\n",
    "    nbins=25,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Computes the sequence lengths dt for each histogram bin.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.core.frame.DataFrame): The tokamak dataframe\n",
    "        denom_dd (scipy.stats._binned_statistic.BinnedStatisticddResult): The denominator histogram.\n",
    "        denom_indices (np.ndarray): The denominator indices.\n",
    "        nbins (int, optional): Number of histogram bins. Defaults to 25.\n",
    "\n",
    "    Returns:\n",
    "        non_dis_time (list): List of arrays of sequence times for non-disrupted data for each bin.\n",
    "        dis_time (list): List of arrays of sequence times for disrupted data for each bin.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dimension Calculation\n",
    "    dimension = len(denom_dd.bin_edges)\n",
    "    if dimension == 1:\n",
    "        binnumber = np.array([denom_dd.binnumber])\n",
    "    else:\n",
    "        binnumber = denom_dd.binnumber\n",
    "    # The : indexes all dims, : indexes the data index, np.newaxis\n",
    "    # prepares the data for np.ix_() to be called\n",
    "    binnumber = binnumber[:, :, np.newaxis]\n",
    "\n",
    "    dt_array = np.zeros(denom_dd.statistic.shape)\n",
    "    \n",
    "    # Store the last entry for continuity checks.\n",
    "    # Here we assume that subsequent dataframe indices\n",
    "    # that are in the same bin must belong to the same \n",
    "    # pulse.\n",
    "    #\n",
    "    # Thus we must check that the last entry and last\n",
    "    # dataframe index number are one and the same.\n",
    "    last_entry = binnumber[:, 0] - 1\n",
    "    dt_integrator = 0\n",
    "    last_dataframe_index = denom_indices[0]-1\n",
    "    last_shot = dataframe[\"shot\"][last_dataframe_index+1]\n",
    "    \n",
    "    # Lists that contain the for loop outputs.\n",
    "    ix_list = []\n",
    "    shot_list = []\n",
    "    dt_list = []\n",
    "    disrupt_list =[]\n",
    "    \n",
    "    # test_array = np.zeros(n_disrupt.shape)\n",
    "    for i in range(denom_dd.binnumber.shape[-1]):\n",
    "        # Get the entry and make sure it is not out of bounds\n",
    "        # For details on this, refer to the notes section of the\n",
    "        # stats.binned_statistic_dd function. Basically bin 0\n",
    "        # and bin nbin+1 are padded bins for outside boundaries\n",
    "        entry = binnumber[:, i] - 1\n",
    "\n",
    "        # Check if the data is out of bounds\n",
    "        if (entry < 0).any() or (entry > nbins - 1).any():\n",
    "            continue\n",
    "\n",
    "        # Get the dataframe entry for this histogram entry\n",
    "        dataframe_index = denom_indices[i]\n",
    "        shot = dataframe[\"shot\"][dataframe_index]\n",
    "        \n",
    "        # Set the is_disrupt flag:\n",
    "        is_disrupt = False\n",
    "        time_until_disrupt_ms = dataframe.time_until_disrupt[dataframe_index] * 1000\n",
    "        if np.abs(time_until_disrupt_ms - tau)<= window:\n",
    "            is_disrupt = True\n",
    "        \n",
    "        # Data continuity check.\n",
    "        # If the current entry is not the last entry\n",
    "        # place all the data into memory and increment\n",
    "        # the entry.\n",
    "        if (entry!=last_entry).all() or dataframe_index != last_dataframe_index+1:\n",
    "            # Convert entry into a struct for array indexing.\n",
    "            ix_entry = np.ix_(*last_entry)\n",
    "            \n",
    "            # List Appends\n",
    "            shot_list.append(last_shot)\n",
    "            ix_list.append(ix_entry)\n",
    "            dt_list.append(dt_integrator)\n",
    "            disrupt_list.append(is_disrupt)\n",
    "            \n",
    "            # Reset the integrator and is_disrupt flags.\n",
    "            last_entry = entry\n",
    "            last_shot = shot\n",
    "            dt_integrator = 0\n",
    "        # Reset the last dataframe index\n",
    "        last_dataframe_index = dataframe_index\n",
    "\n",
    "        # Check if first frame in the shot\n",
    "        # Should be impossible since we exclude ramp ups\n",
    "        if (shot!= dataframe[\"shot\"][dataframe_index - 1]):\n",
    "            continue\n",
    "        # Else: integrate the time\n",
    "        else:\n",
    "            dt_integrator += (\n",
    "                dataframe[\"time\"][dataframe_index]\n",
    "                - dataframe[\"time\"][dataframe_index - 1]\n",
    "            )\n",
    "\n",
    "    # After the loop, convert the lists into a pandas dataframe\n",
    "    pd_dict = {\n",
    "        'shot':shot_list,\n",
    "        'ix':ix_list,\n",
    "        'dt':dt_list,\n",
    "        'is_disrupt':disrupt_list,\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame.from_dict(pd_dict)\n",
    "\n",
    "def entry_list_to_arrays(entry_df, nbins=25):\n",
    "    \n",
    "    # We do this in 1D\n",
    "    non_disrupt = entry_df[entry_df.is_disrupt==False]\n",
    "    disrupt = entry_df[entry_df.is_disrupt==True]\n",
    "    \n",
    "    non_dis_dt_list = []\n",
    "    for i in range(nbins):\n",
    "        # Data selection\n",
    "        non_dis_dt_list.append(np.array(non_disrupt.dt[non_disrupt.ix==([i],)]))\n",
    "        \n",
    "    dis_dt_list = []\n",
    "    for i in range(nbins):\n",
    "        # Data selection\n",
    "        dis_dt_list.append(np.array(disrupt.dt[disrupt.ix==([i],)]))\n",
    "    \n",
    "    return dis_dt_list, non_dis_dt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a71984",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_list = compute_dt_bin(cmod_df, hist, indices_n_total, nbins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_dt_list, non_dis_dt_list = entry_list_to_arrays(entry_list,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca2156",
   "metadata": {},
   "source": [
    "## Optimization Loss Functions\n",
    "Here we do an investigation of the investigation and the corresponding loss functions before we pipe the above data structure into the loss. We do this to make sure it is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07123e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define the function to minimize\n",
    "# This is the log likelihood defined in the document.\n",
    "def z_disruptivity(d: float, dt: np.ndarray):\n",
    "    return (1/d)*(1-np.exp(-dt*d))\n",
    "    \n",
    "def p_data(d:float, dt:np.ndarray, is_disrupt:bool):\n",
    "    \n",
    "    z=z_disruptivity(d, dt)\n",
    "    \n",
    "    if is_disrupt:\n",
    "        return (1 - np.exp(-dt*d)/z)\n",
    "    return (np.exp(-dt*d)/z)\n",
    "\n",
    "def disruptivity_neg_log_likelihood(d:float, dis_dt:np.ndarray, non_dis_dt:np.ndarray,)->float:\n",
    "    \n",
    "    # Safety?\n",
    "    assert d>0, \"Invalid disruptivity, must be greater than 0.\"\n",
    "    \n",
    "    # Compute the renormalization arrays\n",
    "    dis_z = z_disruptivity(d, dis_dt)\n",
    "    non_dis_z = z_disruptivity(d, non_dis_dt)\n",
    "    \n",
    "    # Compute the log likelihood\n",
    "    neg_log_likelihood = \\\n",
    "        - np.sum(np.log(dis_z-np.exp(-dis_dt*d))-np.log(dis_z)) \\\n",
    "        - np.sum((-non_dis_dt*d)-np.log(non_dis_z))\n",
    "    \n",
    "    return neg_log_likelihood\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa6475",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_space = np.linspace(0.01,5,100)\n",
    "dis_dt = np.array([1]*10)\n",
    "non_dis_dt = np.array([1]*10)\n",
    "\n",
    "p_list = []\n",
    "p_dis_list = []\n",
    "p_non_dis_list = []\n",
    "for d in d_space:\n",
    "    p = disruptivity_neg_log_likelihood(d, dis_dt, non_dis_dt)\n",
    "    p_list.append(p)\n",
    "    p_dis_list.append(p_data(d, dis_dt[0], True))\n",
    "    p_non_dis_list.append(p_data(d, non_dis_dt[0], False))\n",
    "    \n",
    "plt.plot(d_space,p_dis_list, label='Disrupted Data')\n",
    "plt.plot(d_space,p_non_dis_list, label='Non Disrupted Data')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"$d$ [1/s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d_space,p_list, label='Negative Log Likelihood')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(\"$d$ [1/s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560973a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = LinearConstraint([1], lb=1e-4)\n",
    "ratio_list = []\n",
    "optimal_d = []\n",
    "n_data = 1000\n",
    "for i in range(n_data):\n",
    "    # Create the dataset\n",
    "    dis_dt = np.array([1]*i)\n",
    "    non_dis_dt = np.array([1]*(n_data-i))\n",
    "    \n",
    "    # Fill the ration\n",
    "    ratio_list.append(i/n_data)\n",
    "    \n",
    "    # Optimize\n",
    "    res = minimize(disruptivity_neg_log_likelihood,\n",
    "                   x0=1,\n",
    "                   args=(dis_dt, non_dis_dt),\n",
    "                   constraints=(constraint)\n",
    "                  )\n",
    "    optimal_d.append(res.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd354228",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ratio_list, optimal_d, label ='Optimal $d$')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel(\"$d$ [1/s]\")\n",
    "plt.xlabel(\"Data Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bd8c7",
   "metadata": {},
   "source": [
    "## Making the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5457b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets pipe this together\n",
    "# something first, efficiency later\n",
    "constraint = LinearConstraint([1], lb=1e-4)\n",
    "optimal_d = []\n",
    "nbins = 25\n",
    "for i in range(nbins):\n",
    "    # Optimize\n",
    "    res = minimize(disruptivity_neg_log_likelihood,\n",
    "                   x0=1,\n",
    "                   args=(dis_dt_list[i], non_dis_dt_list[i]),\n",
    "                   constraints=(constraint)\n",
    "                  )\n",
    "    optimal_d.append(res.x[0])\n",
    "    \n",
    "plot('hello.png', dis_vis.subplot_disruptivity1d, (optimal_d, [0]*nbins, hist.bin_edges, entry_dict_1D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e1cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
